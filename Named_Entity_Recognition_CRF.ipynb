{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named_Entity_Recognition_CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2GxTY1xOd6g"
      },
      "source": [
        "# Named Entity Recognition for Turkish Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxUx-hkpMfAp"
      },
      "source": [
        "##Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANzA7iBf6kMR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "!pip install sklearn-crfsuite\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFwxImqDMlYi"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5W1Yq4U6pb7",
        "outputId": "379b151f-6abb-4ec9-e029-7a96e4dda187"
      },
      "source": [
        "with open('/content/drive/MyDrive/NE.txt') as f:\n",
        "    ne = f.readlines()\n",
        "with open('/content/drive/MyDrive/NE.ma.txt') as f:\n",
        "    ne_ma = f.readlines()\n",
        "\n",
        "print('Size of the dataset:',len(ne))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the dataset: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7_ELvQ7Mp0L"
      },
      "source": [
        "## Split Data into 5 Folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwecXi9s6pg9"
      },
      "source": [
        "fold1, fold2, fold3, fold4, fold5 = [],[],[],[],[]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eif7rvk26pmP"
      },
      "source": [
        "count = 1\n",
        "for line in ne:\n",
        "  if count == 1:\n",
        "    fold1.append(line)\n",
        "    count+=1\n",
        "  elif count == 2:\n",
        "    fold2.append(line)\n",
        "    count+=1\n",
        "  elif count == 3:\n",
        "    fold3.append(line)\n",
        "    count+=1\n",
        "  elif count == 4:\n",
        "    fold4.append(line)\n",
        "    count+=1\n",
        "  elif count == 5:\n",
        "    fold5.append(line)\n",
        "    count = 1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QzS2J566p2K"
      },
      "source": [
        "f1 = pd.DataFrame(fold1, columns=['Lines'])\n",
        "f2 = pd.DataFrame(fold2, columns=['Lines'])\n",
        "f3 = pd.DataFrame(fold3, columns=['Lines'])\n",
        "f4 = pd.DataFrame(fold4, columns=['Lines'])\n",
        "f5 = pd.DataFrame(fold5, columns=['Lines'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRBhWOXAMv4K"
      },
      "source": [
        "## Extracting Labels from Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0q7JO5mM4tg"
      },
      "source": [
        "### 1- Getting Raw Labels with REGEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ejEAMM3yGi"
      },
      "source": [
        "def GetLabel(text):\n",
        "  labelList = re.findall(r'<b_enamex TYPE=\"([A-Z]+)\">([\\w\\s]+)<e_enamex>',text)\n",
        "  labelDict = {}\n",
        "  for label, word in labelList:\n",
        "    labelDict[word] = label\n",
        "  return labelDict"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF0hJhwy4uGz"
      },
      "source": [
        "f1['Labels'] = f1['Lines'].apply(GetLabel)\n",
        "f2['Labels'] = f2['Lines'].apply(GetLabel)\n",
        "f3['Labels'] = f3['Lines'].apply(GetLabel)\n",
        "f4['Labels'] = f4['Lines'].apply(GetLabel)\n",
        "f5['Labels'] = f5['Lines'].apply(GetLabel)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xObU_8DDNEOU"
      },
      "source": [
        "### 2- Getting IOB Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENjXGeAA1mq"
      },
      "source": [
        "def GetIOBLabels(mydict):\n",
        "  bioDict={}\n",
        "  for key, value in mydict.items():\n",
        "    bioList = key.split(' ')\n",
        "    for word in bioList:\n",
        "      if word == bioList[0]:\n",
        "        bioDict[word] = 'B-'+value\n",
        "      else:\n",
        "        bioDict[word] = 'I-'+value\n",
        "  return bioDict"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iMu-oIFBhMa"
      },
      "source": [
        "f1['IOBLabels'] = f1['Labels'].apply(GetIOBLabels)\n",
        "f2['IOBLabels'] = f2['Labels'].apply(GetIOBLabels)\n",
        "f3['IOBLabels'] = f3['Labels'].apply(GetIOBLabels)\n",
        "f4['IOBLabels'] = f4['Labels'].apply(GetIOBLabels)\n",
        "f5['IOBLabels'] = f5['Labels'].apply(GetIOBLabels)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6JqoKDGNK1a"
      },
      "source": [
        "## Importing Location Gazetteer to Add New Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpw8zW-CjAdc"
      },
      "source": [
        "with open('/content/drive/MyDrive/locations_lexicon.txt') as f:\n",
        "    locationList = [line[:-1] for line in f]\n",
        "with open('/content/drive/MyDrive/person_lexicon.txt') as f:\n",
        "    personList = [line[:-1] for line in f]\n",
        "with open('/content/drive/MyDrive/organization_lexicon.txt') as f:\n",
        "    organizationList = [line[:-1] for line in f]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19o_3id9jhT3"
      },
      "source": [
        "def LocationLexicon(text):\n",
        "  if text in locationList:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def PersonLexicon(text):\n",
        "  if text in personList:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def OrganizationLexicon(text):\n",
        "  if text in organizationList:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALh4gNjFNphN"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5pf1Y4W6p7l"
      },
      "source": [
        "ne_df = pd.DataFrame(ne_ma)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTNG53fjOKmU"
      },
      "source": [
        "def getLineNum(text):\n",
        "  sepList = text.split(' ')\n",
        "  return sepList[0]\n",
        "\n",
        "def getWord(text):\n",
        "  if text.split(' ')[1] != \"'\":\n",
        "    sepList = text.split(' ')\n",
        "    thewordraw = sepList[1]\n",
        "    splitted = thewordraw.split(\"'\")\n",
        "    if splitted[0]=='':\n",
        "      return splitted[1]\n",
        "    else:\n",
        "      return splitted[0]\n",
        "  else:\n",
        "    return \"' '\"\n",
        "\n",
        "def getInflectionalPart(text):\n",
        "  sepList = text.split(' ')\n",
        "  infText = sepList[2]\n",
        "  return infText[:-1]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F247eklqOmt0"
      },
      "source": [
        "ne_df['LineNum'] = ne_df[0].apply(getLineNum)\n",
        "ne_df['Word'] = ne_df[0].apply(getWord)\n",
        "ne_df['Inflectional'] = ne_df[0].apply(getInflectionalPart)\n",
        "\n",
        "# Gazetteer Features\n",
        "ne_df['LocationLexicon'] = ne_df['Word'].apply(LocationLexicon)\n",
        "ne_df['PersonLexicon'] = ne_df['Word'].apply(PersonLexicon)\n",
        "ne_df['OrganizationLexicon'] = ne_df['Word'].apply(OrganizationLexicon)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbaR8vNKPslT"
      },
      "source": [
        "def GetRoot(text):\n",
        "  splitList = text.split('+')\n",
        "  return splitList[0].lower()\n",
        "\n",
        "def GetNounCase(text):\n",
        "  splitText = text.split('+')\n",
        "  if 'Noun' in splitText:\n",
        "    return splitText[-1]\n",
        "  else:\n",
        "    return '0'\n",
        "\n",
        "def PropExist(text):\n",
        "  splitText = text.split('+')\n",
        "  if 'Prop' in splitText:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def GetPOS(text):\n",
        "  splitText = text.split('+')\n",
        "  if text != '*UNKNOWN*':\n",
        "    if splitText[1] == 'Verb^DB':\n",
        "      return 'Verb'\n",
        "    elif splitText[1] == 'Adj^DB':\n",
        "      return 'Adj'\n",
        "    else:\n",
        "      return splitText[1]\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "def GetAllInflectional(text):\n",
        "  splitText = text.split('+')\n",
        "  if text != '*UNKNOWN*':\n",
        "    index = 0\n",
        "    for word in splitText:\n",
        "      if word == 'Verb^DB' or word == 'Adj^DB':\n",
        "        index+=1\n",
        "        break\n",
        "      elif word in ['Noun', 'Adj', 'Conj', 'Verb', 'Num', 'Punct','Postp', 'Det', 'Adverb', 'Interj', 'Pron', 'Ques','Dup']:\n",
        "        break\n",
        "      index+=1\n",
        "    if len(splitText)==index+1:\n",
        "      return '0'\n",
        "    else:\n",
        "      return '+'.join(splitText[index+1:])\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "def GetCase(text):\n",
        "  if text[0].isupper() == True:\n",
        "    return 'UC'\n",
        "  else:\n",
        "    return 'DC'"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYvyCeTPmdX"
      },
      "source": [
        "ne_df['StartOfSent'] = 0\n",
        "\n",
        "for row in range(1,ne_df.shape[0]):\n",
        "  prev_val = ne_df.loc[row-1,'LineNum']\n",
        "  curr_val = ne_df.loc[row,'LineNum']\n",
        "  if prev_val != curr_val:\n",
        "    ne_df.loc[row,'StartOfSent'] = 1\n",
        "\n",
        "ne_df.loc[0,'StartOfSent'] = 1\n",
        "\n",
        "ne_df['Root'] = ne_df['Inflectional'].apply(GetRoot)\n",
        "ne_df['NounCase'] = ne_df['Inflectional'].apply(GetNounCase)\n",
        "ne_df['PropExist'] = ne_df['Inflectional'].apply(PropExist)\n",
        "ne_df['POS'] = ne_df['Inflectional'].apply(GetPOS)\n",
        "ne_df['AllInflectional'] = ne_df['Inflectional'].apply(GetAllInflectional)\n",
        "ne_df['Case'] = ne_df['Word'].apply(GetCase)\n",
        "ne_df['Label'] = ''"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FHok11RN2DZ"
      },
      "source": [
        "### Assigning Labels to Tokens in ne_df Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph-hW8RIDfhm"
      },
      "source": [
        "def LabellingFolds(ne_df, fold, foldnum):\n",
        "  count_index = 0 # For fold iteration\n",
        "  for ind in np.arange(foldnum,10000,5): # 1, 6, 11 ... for fold1\n",
        "    for w_index in list(ne_df[ne_df['LineNum']==str(ind)].index):\n",
        "      theword = ne_df.loc[w_index,'Word'] # Get the word to find its label\n",
        "      thedict = fold.loc[count_index,'IOBLabels'] # Get the labels of this sentence\n",
        "      if theword in thedict.keys(): # Check if that word is in the labels\n",
        "        ne_df.loc[w_index,'Label'] = thedict[theword] # If exist, assign its label\n",
        "      else:\n",
        "        ne_df.loc[w_index,'Label'] = 'O' # If not assign 'O'\n",
        "    count_index+=1\n",
        "  print('Fold',foldnum,'Ends')\n",
        "  return ne_df"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqMlMJf_VXAj",
        "outputId": "9dfee98a-b4cb-4566-b722-8300e1713efb"
      },
      "source": [
        "ne_df = LabellingFolds(ne_df, f1, 1)\n",
        "ne_df = LabellingFolds(ne_df, f2, 2)\n",
        "ne_df = LabellingFolds(ne_df, f3, 3)\n",
        "ne_df = LabellingFolds(ne_df, f4, 4)\n",
        "ne_df = LabellingFolds(ne_df, f5, 5)\n",
        "ne_df.loc[165847,'Label']='O'"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 Ends\n",
            "Fold 2 Ends\n",
            "Fold 3 Ends\n",
            "Fold 4 Ends\n",
            "Fold 5 Ends\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4VFYhXqOHgd"
      },
      "source": [
        "## Creating Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_eIt60dZbxZ"
      },
      "source": [
        "def createTrainData(ne_df,foldnum, features):\n",
        "  foldList_X = []\n",
        "  foldList_y = []\n",
        "  for ind in np.arange(foldnum,10000,5):\n",
        "    tr_list = ne_df[ne_df['LineNum']==str(ind)].loc[:,features].to_dict('records')\n",
        "    tr_list2 = list(ne_df[ne_df['LineNum']==str(ind)].loc[:,'Label'])\n",
        "    foldList_X.append(tr_list)\n",
        "    foldList_y.append(tr_list2)\n",
        "  print('Function Worked for Fold Number {}'.format(foldnum))\n",
        "  return foldList_X, foldList_y"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38dmpa6m2uGc",
        "outputId": "94c4e4b8-582f-4fe3-e7ae-e5f479d1e4b0"
      },
      "source": [
        "X_train_fold1, y_train_fold1 = createTrainData(ne_df,foldnum=1, features = ['StartOfSent','Root','NounCase','PropExist','POS','AllInflectional','LocationLexicon','PersonLexicon','OrganizationLexicon','Case'])\n",
        "X_train_fold2, y_train_fold2 = createTrainData(ne_df,foldnum=2, features = ['StartOfSent','Root','NounCase','PropExist','POS','AllInflectional','LocationLexicon','PersonLexicon','OrganizationLexicon','Case'])\n",
        "X_train_fold3, y_train_fold3 = createTrainData(ne_df,foldnum=3, features = ['StartOfSent','Root','NounCase','PropExist','POS','AllInflectional','LocationLexicon','PersonLexicon','OrganizationLexicon','Case'])\n",
        "X_train_fold4, y_train_fold4 = createTrainData(ne_df,foldnum=4, features = ['StartOfSent','Root','NounCase','PropExist','POS','AllInflectional','LocationLexicon','PersonLexicon','OrganizationLexicon','Case'])\n",
        "X_train_fold5, y_train_fold5 = createTrainData(ne_df,foldnum=5, features = ['StartOfSent','Root','NounCase','PropExist','POS','AllInflectional','LocationLexicon','PersonLexicon','OrganizationLexicon','Case'])\n",
        "\n",
        "X_list = [X_train_fold1,X_train_fold2,X_train_fold3,X_train_fold4,X_train_fold5]\n",
        "y_list = [y_train_fold1,y_train_fold2,y_train_fold3,y_train_fold4,y_train_fold5]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function Worked for Fold Number 1\n",
            "Function Worked for Fold Number 2\n",
            "Function Worked for Fold Number 3\n",
            "Function Worked for Fold Number 4\n",
            "Function Worked for Fold Number 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xnLUOiLOL8T"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdUCMjqGc1hr"
      },
      "source": [
        "def calculateFold(listX, listy,foldnum,c1,c2,max_iter):\n",
        "  if foldnum == 1:\n",
        "    X_train = listX[1] + listX[2] + listX[3] + listX[4]\n",
        "    y_train = listy[1] + listy[2] + listy[3] + listy[4]\n",
        "    X_test = listX[0]\n",
        "    y_test = listy[0]\n",
        "  elif foldnum == 2:\n",
        "    X_train = listX[0] + listX[2] + listX[3] + listX[4]\n",
        "    y_train = listy[0] + listy[2] + listy[3] + listy[4]\n",
        "    X_test = listX[1]\n",
        "    y_test = listy[1]\n",
        "  elif foldnum == 3:\n",
        "    X_train = listX[0] + listX[1] + listX[3] + listX[4]\n",
        "    y_train = listy[0] + listy[1] + listy[3] + listy[4]\n",
        "    X_test = listX[2]\n",
        "    y_test = listy[2]\n",
        "  elif foldnum == 4:\n",
        "    X_train = listX[0] + listX[1] + listX[2] + listX[4]\n",
        "    y_train = listy[0] + listy[1] + listy[2] + listy[4]\n",
        "    X_test = listX[3]\n",
        "    y_test = listy[3]\n",
        "  elif foldnum == 5:\n",
        "    X_train = listX[0] + listX[1] + listX[2] + listX[3]\n",
        "    y_train = listy[0] + listy[1] + listy[2] + listy[3]\n",
        "    X_test = listX[4]\n",
        "    y_test = listy[4]\n",
        "\n",
        "  crf = sklearn_crfsuite.CRF(\n",
        "  algorithm='lbfgs',\n",
        "  c1=c1,\n",
        "  c2=c2,\n",
        "  max_iterations=max_iter,\n",
        "  all_possible_transitions=True)\n",
        "  crf.fit(X_train, y_train)\n",
        "\n",
        "  my_labels = list(crf.classes_)\n",
        "  my_labels.remove('O')\n",
        "\n",
        "  y_pred = crf.predict(X_test)\n",
        "  f1_score = metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=my_labels)\n",
        "  precision = metrics.flat_precision_score(y_test, y_pred,average='weighted', labels=my_labels)\n",
        "  recall = metrics.flat_recall_score(y_test, y_pred,average='weighted', labels=my_labels)\n",
        "  sorted_labels = sorted(my_labels,key=lambda name: (name[1:], name[0]))\n",
        "  print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
        "  return f1_score, precision, recall"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViHGYkB7NOD7",
        "outputId": "e5c8dc0b-84ae-439c-9be3-45b97f064583"
      },
      "source": [
        "fold1_f1, fold1_precision, fold1_recall = calculateFold(X_list, y_list,foldnum=1, c1=0.02, c2=0.02, max_iter=100)\n",
        "fold2_f1, fold2_precision, fold2_recall = calculateFold(X_list, y_list,foldnum=2, c1=0.02, c2=0.02, max_iter=100)\n",
        "fold3_f1, fold3_precision, fold3_recall = calculateFold(X_list, y_list,foldnum=3, c1=0.02, c2=0.02, max_iter=100)\n",
        "fold4_f1, fold4_precision, fold4_recall = calculateFold(X_list, y_list,foldnum=4, c1=0.02, c2=0.02, max_iter=100)\n",
        "fold5_f1, fold5_precision, fold5_recall = calculateFold(X_list, y_list,foldnum=5, c1=0.02, c2=0.02, max_iter=100)\n",
        "\n",
        "print('Average F1 Score:', (fold1_f1+fold2_f1+fold3_f1+fold4_f1+fold5_f1)/5)\n",
        "print('Average Precision:', (fold1_precision+fold2_precision+fold3_precision+fold4_precision+fold5_precision)/5)\n",
        "print('Average Recall:', (fold1_recall+fold2_recall+fold3_recall+fold4_recall+fold5_recall)/5)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "    B-LOCATION      0.933     0.909     0.921       836\n",
            "    I-LOCATION      0.658     0.575     0.613        87\n",
            "B-ORGANIZATION      0.900     0.835     0.866       624\n",
            "I-ORGANIZATION      0.805     0.764     0.784       351\n",
            "      B-PERSON      0.920     0.889     0.904      1059\n",
            "      I-PERSON      0.869     0.888     0.878       439\n",
            "\n",
            "     micro avg      0.895     0.863     0.878      3396\n",
            "     macro avg      0.847     0.810     0.828      3396\n",
            "  weighted avg      0.894     0.863     0.878      3396\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    B-LOCATION      0.937     0.914     0.925       747\n",
            "    I-LOCATION      0.720     0.771     0.745        70\n",
            "B-ORGANIZATION      0.885     0.828     0.856       547\n",
            "I-ORGANIZATION      0.792     0.784     0.788       306\n",
            "      B-PERSON      0.903     0.867     0.885      1053\n",
            "      I-PERSON      0.874     0.877     0.876       457\n",
            "\n",
            "     micro avg      0.888     0.863     0.875      3180\n",
            "     macro avg      0.852     0.840     0.846      3180\n",
            "  weighted avg      0.889     0.863     0.876      3180\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    B-LOCATION      0.938     0.920     0.929       800\n",
            "    I-LOCATION      0.846     0.632     0.724        87\n",
            "B-ORGANIZATION      0.896     0.862     0.879       581\n",
            "I-ORGANIZATION      0.850     0.788     0.818       353\n",
            "      B-PERSON      0.913     0.868     0.890      1095\n",
            "      I-PERSON      0.843     0.895     0.868       437\n",
            "\n",
            "     micro avg      0.898     0.868     0.883      3353\n",
            "     macro avg      0.881     0.827     0.851      3353\n",
            "  weighted avg      0.899     0.868     0.882      3353\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    B-LOCATION      0.943     0.901     0.921       895\n",
            "    I-LOCATION      0.791     0.673     0.727       107\n",
            "B-ORGANIZATION      0.916     0.853     0.883       617\n",
            "I-ORGANIZATION      0.792     0.788     0.790       363\n",
            "      B-PERSON      0.898     0.867     0.882      1129\n",
            "      I-PERSON      0.800     0.876     0.836       442\n",
            "\n",
            "     micro avg      0.885     0.860     0.872      3553\n",
            "     macro avg      0.857     0.826     0.840      3553\n",
            "  weighted avg      0.886     0.860     0.872      3553\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    B-LOCATION      0.937     0.884     0.910       853\n",
            "    I-LOCATION      0.822     0.588     0.686       102\n",
            "B-ORGANIZATION      0.895     0.849     0.872       564\n",
            "I-ORGANIZATION      0.779     0.764     0.771       318\n",
            "      B-PERSON      0.886     0.861     0.874      1155\n",
            "      I-PERSON      0.858     0.880     0.869       508\n",
            "\n",
            "     micro avg      0.884     0.851     0.867      3500\n",
            "     macro avg      0.863     0.805     0.830      3500\n",
            "  weighted avg      0.884     0.851     0.867      3500\n",
            "\n",
            "Average F1 Score: 0.8749984355670515\n",
            "Average Precision: 0.8903999216117331\n",
            "Average Recall: 0.8609651853441391\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}